from src.mlops.abstractions import ModelABC
from src.mlops.model.model_utils import mlflow_insample_metrics_log
from itertools import product
from joblib import Parallel, delayed
from typing import Union, Tuple, List
import pandas as pd
import numpy as np
from tqdm import tqdm
from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import (
    roc_auc_score, f1_score, precision_score, recall_score,
    make_scorer, log_loss
)
from sklearn.utils.class_weight import compute_class_weight
import lightgbm as lgb
from lightgbm import LGBMClassifier
from lightgbm.callback import early_stopping
import mlflow
import mlflow.lightgbm
from mlflow.models import infer_signature

import os
os.environ['CUDA_VISIBLE_DEVICES'] = "1"
class LGBClassifier(ModelABC):

    def train(self,
              train_val_splits: List,
              ):

        # ==================== <START> Enable autologging ====================
        mlflow.lightgbm.autolog()
        # ==================== <END> Enable autologging ====================

        # ==================== <START> Prepare data for training without cross validation ====================
        X_train = train_val_splits[-1]['train'].drop(columns=['Y'])
        y_train = train_val_splits[-1]['train']['Y']
        X_valid = None
        y_valid = None

        # Prepare the dataset for LightGBM
        classes = np.array([0, 1, 2])
        class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)
        class_weights_dict = dict(zip(classes, class_weights))
        sample_weights = y_train.map(class_weights_dict)
        train_data = lgb.Dataset(X_train, label=y_train, weight=sample_weights)

        # ==================== <END> Prepare data for training without cross validation ====================

        # ==================== <START> Model configuration ====================
        # valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)
        # multiclass_roc_auc = functools.partial(roc_auc_score, multi_class='ovr')
        # roc_auc_scorer = make_scorer(multiclass_roc_auc, needs_proba=True)

        # Set LightGBM parameters
        params = {
            'objective': 'multiclass',
            'num_class': y_train.nunique(),
            'max_bin': 63,
            'boosting_type': 'gbdt',
            'device': 'gpu',
            'metric': 'multi_logloss',
            'num_leaves': 31,
            'learning_rate': 0.02,
            'bagging_fraction': 1.0,
            'bagging_freq': 5,
            'gpu_platform_id': 0,
            'gpu_device_id': 2
        }

        grid_params = {
            'n_estimators': [100],
            'num_leaves': [31, 50, 100],
            'learning_rate': [0.01, 0.05, 0.1]
        }
        # ==================== <END> Model configuration ====================

        # Adjust cv based on the size of X_train
        cv = min(2, len(X_train))  # Ensure cv is not greater than the number of samples

        # ===== Start MLflow Run & Train with GridSearchCV (No Validation Data Used Yet) =====
        with mlflow.start_run(run_name=f"{self.__class__.__name__}_Multiclass", nested=True) as run:
            if cv >= 2:
                # Create the regressor
                mod = lgb.LGBMClassifier(**params)

                grid_search = RandomizedSearchCV(
                    estimator=mod,
                    param_distributions=grid_params,
                    n_iter=3,
                    cv=StratifiedKFold(n_splits=cv, shuffle=True, random_state=42),
                    n_jobs=-1,
                    verbose=1,
                    random_state=42,
                    scoring=roc_auc_scorer,
                )
                grid_search.fit(X_train, y_train)

                # Get the best parameters and log them
                # best_params = grid_search.best_params_
                # mlflow.log_params(best_params)

                # Log metrics from cross-validation
                best_score = grid_search.best_score_
                if best_score is not None:
                    mlflow.log_metric("best_auc_randomCV", best_score)

                # ===== Train Final Model Using Train + Validation Data =====
                # final_model = lgb.LGBMClassifier(**{**best_params, 'n_estimators': 500})
                #
                # # Train the final model with early stopping (Fixed)
                # final_model.fit(
                #     X_train, y_train,
                #     eval_set=[(X_valid, y_valid)],
                #     callbacks=[early_stopping(50)]  # Use this instead of early_stopping_rounds
                # )
                final_model = grid_search.best_estimator_

                mlflow_insample_metrics_log(final_model, X_train, y_train)

            else:
                # # Not enough data for cross-validation; fit the model directly
                # mod.fit(X_train, y_train)
                # model = mod  # Assign the directly fitted model as the best model
                # mlflow.log_params(params)
                # params = {
                #     'objective': 'multiclass',
                #     'num_class': y_train.nunique(),
                #     'boosting_type': 'gbdt',
                #     'device': 'gpu',
                #     'metric': 'multi_logloss',
                #     'gpu_use_dp': False,
                #     'max_bin': 63
                # }

                final_model = lgb.train(
                    params,
                    train_data,
                    num_boost_round=100,
                    # valid_sets=[train_data, valid_data],
                    # valid_names=['train', 'valid'],
                    # callbacks=[early_stopping(50)]
                )

                mlflow_insample_metrics_log(final_model, X_train, y_train)

                # ## Logging in sample roc_auc
                # # === In-sample predictions ===
                # y_proba = final_model.predict(X_train)
                # y_pred = np.argmax(y_proba, axis=1)
                # # y_proba = final_model.predict_proba(X_train)
                #
                # # === Compute metrics - ovr ===
                # average = 'macro'
                # in_sample_roc_auc = roc_auc_score(y_train, y_proba, multi_class='ovr')
                # in_sample_f1 = f1_score(y_train, y_pred, average=average)
                # in_sample_precision = precision_score(y_train, y_pred, average=average)
                # in_sample_recall = recall_score(y_train, y_pred, average=average)
                #
                # df = pd.DataFrame({"y": y_train, "y_pred": y_proba[:, 1]})
                # df = df[df.y <= 1]
                # df = df[df.y_pred > 0]
                # auc = roc_auc_score(df['y'], df['y_pred'])
                # in_sample_ar = 2 * auc - 1
                #
                # # === Log metrics - ovr ===
                # mlflow.log_metric("insample_ovr_roc_auc", in_sample_roc_auc)
                # mlflow.log_metric(f"insample_ovr_f1_{average}", in_sample_f1)
                # mlflow.log_metric(f"insample_ovr_precision_{average}", in_sample_precision)
                # mlflow.log_metric(f"insample_ovr_recall_{average}", in_sample_recall)
                # mlflow.log_metric("insample_ovr_ar", in_sample_ar)
                #
                # # === Compute metrics - ovo ===
                # eval_01 = evaluate_ovo(y_train, y_pred, y_proba, 0, 1)
                # eval_02 = evaluate_ovo(y_train, y_pred, y_proba, 0, 2)
                #
                # for classvs, value in eval_01.items():
                #     for metric, metric_value in value.items():
                #         print(f'{classvs}_{metric}', metric_value)
                #         mlflow.log_metric(f'insample_{classvs}{metric}', metric_value)
                #
                # for classvs, value in eval_02.items():
                #     for metric, metric_value in value.items():
                #         print(f'{classvs}_{metric}', metric_value)
                #         mlflow.log_metric(f'insample_{classvs}{metric}', metric_value)



            # Evaluate model on validation set
            # y_valid_prob = final_model.predict(X_valid)

            # # Convert probabilities to binary labels (Threshold = 0.5)
            # y_valid_pred = (y_valid_prob > 0.5).astype(int)
            #
            # valid_accuracy = accuracy_score(y_valid, y_valid_pred)
            # valid_auc = roc_auc_score(y_valid, y_valid_pred)

            # # Log validation metrics
            # mlflow.log_metric("valid_accuracy", valid_accuracy)
            # mlflow.log_metric("valid_auc", valid_auc)

            signature = infer_signature(X_train.head(100), final_model.predict(X_train.head(100)))
            mlflow.lightgbm.log_model(final_model, "model", signature=signature)
            # Log the model to MLflow
            model_name = f"{self.__class__.__name__}"
            model_run_id = run.info.run_id

        return final_model, model_name, model_run_id